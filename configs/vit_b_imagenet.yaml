# ImageNet-1K Training Configuration for ViT-B with FAORU

# Model
model:
  name: vit_base_patch16_224
  num_classes: 1000
  drop_rate: 0.0
  drop_path_rate: 0.1
  
  # FAORU settings
  faoru:
    enabled: true
    variant: learnable  # 'piecewise', 'smooth', 'learnable'
    transform: fft      # 'fft', 'dct', 'hadamard'
    epsilon: 1e-6
    cutoff_ratio: 0.3
    lambda_low: 0.3
    lambda_high: 1.0

# Data
data:
  dataset: imagenet
  data_path: /path/to/imagenet
  batch_size: 128  # per GPU, effective batch = 128 * 8 = 1024
  num_workers: 8
  pin_memory: true
  
  # Augmentation
  input_size: 224
  crop_pct: 0.875
  interpolation: bicubic
  
  # Training augmentation
  train_aug:
    auto_augment: rand-m9-mstd0.5-inc1
    random_erasing_prob: 0.25
    mixup: 0.8
    cutmix: 1.0
    mixup_switch_prob: 0.5
    mixup_mode: batch
    
  # Color jitter
  color_jitter: 0.4
  
  # Normalization
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]

# Training
training:
  epochs: 400
  warmup_epochs: 5
  
  # Optimizer
  optimizer: adamw
  lr: 3e-4
  weight_decay: 0.05
  momentum: 0.9  # for SGD
  
  # Learning rate schedule
  lr_scheduler: cosine
  min_lr: 1e-6
  warmup_lr: 1e-6
  
  # Loss
  label_smoothing: 0.1
  
  # Gradient clipping
  clip_grad: 1.0
  
  # Mixed precision
  amp: true
  amp_dtype: bfloat16  # or float16

# Validation
validation:
  batch_size: 256
  crop_pct: 0.9
  num_crops: 1

# Checkpointing
checkpoint:
  save_freq: 10  # save every N epochs
  keep_last: 5   # keep last N checkpoints
  save_best: true

# Logging
logging:
  log_freq: 50  # log every N iterations
  use_tensorboard: true
  use_wandb: false
  project_name: faoru-imagenet
  
# System
system:
  seed: 42
  deterministic: false
  cudnn_benchmark: true
  num_gpus: 8
  
# Distributed training
distributed:
  backend: nccl
  find_unused_parameters: false
  
# Reproducibility seeds for 5-run experiments
seeds: [42, 1337, 2024, 3141, 9999]
